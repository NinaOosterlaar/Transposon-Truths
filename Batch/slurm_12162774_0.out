Task 0: Training ZINBAE
Device: cuda
Loading training data from: Data/processed_data/BinSize10_MovingAvgFalse_ZINB_train_data.npy
Applied denoising with 30.0% masking
Loading test data from: Data/processed_data/BinSize10_MovingAvgFalse_ZINB_test_data.npy
Applied denoising with 30.0% masking
Number of training samples: 14824
============================================================
TRAINING ZINB AUTOENCODER (ZINBAE)
============================================================
Epoch [1/100], Total Loss: 4.8225, NLL: 2.0397, Masked: 2.7827
Epoch [2/100], Total Loss: 4.4357, NLL: 1.9426, Masked: 2.4931
Epoch [3/100], Total Loss: 4.4224, NLL: 1.9341, Masked: 2.4883
Epoch [4/100], Total Loss: 4.4011, NLL: 1.8987, Masked: 2.5024
Epoch [5/100], Total Loss: 4.3268, NLL: 1.8721, Masked: 2.4548
Epoch [6/100], Total Loss: 4.3089, NLL: 1.8609, Masked: 2.4480
Epoch [7/100], Total Loss: 4.2734, NLL: 1.8467, Masked: 2.4267
Epoch [8/100], Total Loss: 4.2454, NLL: 1.8338, Masked: 2.4116
Epoch [9/100], Total Loss: 4.2440, NLL: 1.8272, Masked: 2.4168
Epoch [10/100], Total Loss: 4.2208, NLL: 1.8237, Masked: 2.3971
Epoch [11/100], Total Loss: 4.2180, NLL: 1.8241, Masked: 2.3939
Epoch [12/100], Total Loss: 4.1983, NLL: 1.8165, Masked: 2.3817
Epoch [13/100], Total Loss: 4.1817, NLL: 1.8114, Masked: 2.3703
Epoch [14/100], Total Loss: 20.4286, NLL: 1.8101, Masked: 18.6185
Epoch [15/100], Total Loss: 4.1581, NLL: 1.8034, Masked: 2.3547
Epoch [16/100], Total Loss: 4.1445, NLL: 1.7993, Masked: 2.3452
Epoch [17/100], Total Loss: 4.1381, NLL: 1.7973, Masked: 2.3407
Epoch [18/100], Total Loss: 4.1256, NLL: 1.7941, Masked: 2.3315
Epoch [19/100], Total Loss: 4.1117, NLL: 1.7908, Masked: 2.3209
Epoch [20/100], Total Loss: 4.1005, NLL: 1.7880, Masked: 2.3125
Epoch [21/100], Total Loss: 4.0977, NLL: 1.7845, Masked: 2.3132
Epoch [22/100], Total Loss: 4.0845, NLL: 1.7812, Masked: 2.3034
Epoch [23/100], Total Loss: 4.0844, NLL: 1.7795, Masked: 2.3049
Epoch [24/100], Total Loss: 4.0630, NLL: 1.7756, Masked: 2.2875
Epoch [25/100], Total Loss: 4.0544, NLL: 1.7732, Masked: 2.2813
Epoch [26/100], Total Loss: 4.0430, NLL: 1.7704, Masked: 2.2726
Epoch [27/100], Total Loss: 4.0307, NLL: 1.7667, Masked: 2.2640
Epoch [28/100], Total Loss: 4.0217, NLL: 1.7656, Masked: 2.2561
Epoch [29/100], Total Loss: 4.0070, NLL: 1.7631, Masked: 2.2439
Epoch [30/100], Total Loss: 4.0035, NLL: 1.7618, Masked: 2.2418
Epoch [31/100], Total Loss: 3.9936, NLL: 1.7588, Masked: 2.2348
Epoch [32/100], Total Loss: 3.9930, NLL: 1.7582, Masked: 2.2348
Epoch [33/100], Total Loss: 3.9806, NLL: 1.7556, Masked: 2.2250
Epoch [34/100], Total Loss: 3.9695, NLL: 1.7542, Masked: 2.2152
Epoch [35/100], Total Loss: 3.9691, NLL: 1.7529, Masked: 2.2162
Epoch [36/100], Total Loss: 3.9577, NLL: 1.7515, Masked: 2.2062
Epoch [37/100], Total Loss: 3.9500, NLL: 1.7501, Masked: 2.1999
Epoch [38/100], Total Loss: 3.9406, NLL: 1.7475, Masked: 2.1931
Epoch [39/100], Total Loss: 3.9280, NLL: 1.7462, Masked: 2.1819
Epoch [40/100], Total Loss: 3.9374, NLL: 1.7451, Masked: 2.1923
Epoch [41/100], Total Loss: 3.9181, NLL: 1.7441, Masked: 2.1740
Epoch [42/100], Total Loss: 3.9171, NLL: 1.7429, Masked: 2.1742
Epoch [43/100], Total Loss: 3.9095, NLL: 1.7418, Masked: 2.1677
Epoch [44/100], Total Loss: 3.9005, NLL: 1.7403, Masked: 2.1602
Epoch [45/100], Total Loss: 3.8942, NLL: 1.7395, Masked: 2.1548
Epoch [46/100], Total Loss: 3.8879, NLL: 1.7380, Masked: 2.1499
Epoch [47/100], Total Loss: 3.8811, NLL: 1.7367, Masked: 2.1444
Epoch [48/100], Total Loss: 3.8724, NLL: 1.7355, Masked: 2.1369
Epoch [49/100], Total Loss: 3.8693, NLL: 1.7348, Masked: 2.1345
Epoch [50/100], Total Loss: 3.8630, NLL: 1.7338, Masked: 2.1293
Epoch [51/100], Total Loss: 3.8597, NLL: 1.7325, Masked: 2.1272
Epoch [52/100], Total Loss: 3.8505, NLL: 1.7317, Masked: 2.1189
Epoch [53/100], Total Loss: 3.8440, NLL: 1.7306, Masked: 2.1134
Epoch [54/100], Total Loss: 3.8385, NLL: 1.7295, Masked: 2.1090
Epoch [55/100], Total Loss: 3.8364, NLL: 1.7281, Masked: 2.1083
Epoch [56/100], Total Loss: 3.8297, NLL: 1.7277, Masked: 2.1020
Epoch [57/100], Total Loss: 3.8305, NLL: 1.7270, Masked: 2.1035
Epoch [58/100], Total Loss: 3.8258, NLL: 1.7262, Masked: 2.0997
Epoch [59/100], Total Loss: 3.8172, NLL: 1.7250, Masked: 2.0922
Epoch [60/100], Total Loss: 3.8114, NLL: 1.7238, Masked: 2.0876
Epoch [61/100], Total Loss: 3.8049, NLL: 1.7227, Masked: 2.0821
Epoch [62/100], Total Loss: 3.8000, NLL: 1.7215, Masked: 2.0785
Epoch [63/100], Total Loss: 3.7951, NLL: 1.7210, Masked: 2.0741
Epoch [64/100], Total Loss: 3.7886, NLL: 1.7197, Masked: 2.0689
Epoch [65/100], Total Loss: 3.7859, NLL: 1.7191, Masked: 2.0667
Epoch [66/100], Total Loss: 3.7832, NLL: 1.7181, Masked: 2.0651
Epoch [67/100], Total Loss: 3.7793, NLL: 1.7174, Masked: 2.0619
Epoch [68/100], Total Loss: 3.7740, NLL: 1.7173, Masked: 2.0567
Epoch [69/100], Total Loss: 3.7672, NLL: 1.7153, Masked: 2.0518
Epoch [70/100], Total Loss: 3.7643, NLL: 1.7148, Masked: 2.0495
Epoch [71/100], Total Loss: 3.7628, NLL: 1.7139, Masked: 2.0489
Epoch [72/100], Total Loss: 3.7559, NLL: 1.7131, Masked: 2.0427
Epoch [73/100], Total Loss: 3.7492, NLL: 1.7120, Masked: 2.0372
Epoch [74/100], Total Loss: 3.7467, NLL: 1.7113, Masked: 2.0354
Epoch [75/100], Total Loss: 3.7472, NLL: 1.7104, Masked: 2.0368
Epoch [76/100], Total Loss: 3.7373, NLL: 1.7095, Masked: 2.0278
Epoch [77/100], Total Loss: 3.7342, NLL: 1.7086, Masked: 2.0256
Epoch [78/100], Total Loss: 3.7294, NLL: 1.7081, Masked: 2.0213
Epoch [79/100], Total Loss: 3.7214, NLL: 1.7066, Masked: 2.0148
Epoch [80/100], Total Loss: 3.7209, NLL: 1.7062, Masked: 2.0147
Epoch [81/100], Total Loss: 3.7210, NLL: 1.7057, Masked: 2.0152
Epoch [82/100], Total Loss: 3.7123, NLL: 1.7043, Masked: 2.0080
Epoch [83/100], Total Loss: 3.7081, NLL: 1.7036, Masked: 2.0045
Epoch [84/100], Total Loss: 3.7032, NLL: 1.7024, Masked: 2.0008
Epoch [85/100], Total Loss: 3.7002, NLL: 1.7016, Masked: 1.9986
Epoch [86/100], Total Loss: 3.6954, NLL: 1.7006, Masked: 1.9948
Epoch [87/100], Total Loss: 3.6896, NLL: 1.7002, Masked: 1.9894
Epoch [88/100], Total Loss: 3.6896, NLL: 1.6996, Masked: 1.9900
Epoch [89/100], Total Loss: 3.6835, NLL: 1.6988, Masked: 1.9847
Epoch [90/100], Total Loss: 3.6822, NLL: 1.6981, Masked: 1.9840
Epoch [91/100], Total Loss: 3.6759, NLL: 1.6976, Masked: 1.9783
Epoch [92/100], Total Loss: 3.6716, NLL: 1.6965, Masked: 1.9752
Epoch [93/100], Total Loss: 3.6684, NLL: 1.6958, Masked: 1.9726
Epoch [94/100], Total Loss: 3.6686, NLL: 1.6954, Masked: 1.9733
Epoch [95/100], Total Loss: 3.6652, NLL: 1.6948, Masked: 1.9704
Epoch [96/100], Total Loss: 3.6583, NLL: 1.6940, Masked: 1.9643
Epoch [97/100], Total Loss: 3.6550, NLL: 1.6934, Masked: 1.9616
Epoch [98/100], Total Loss: 3.6557, NLL: 1.6929, Masked: 1.9628
Epoch [99/100], Total Loss: 3.6489, NLL: 1.6918, Masked: 1.9571
Epoch [100/100], Total Loss: 3.6474, NLL: 1.6912, Masked: 1.9562
Training loss plot saved to /workspace/AE/results/training/this_is_exp+1/this_is_exp+1_ZINBAE_20260119_140639_conv_training_losses.png
Training losses saved to /workspace/AE/results/training/this_is_exp+1/this_is_exp+1_ZINBAE_20260119_140639_conv_training_losses.json

==================================================
EVALUATING ON TRAINING DATA
==================================================

==================================================
TEST RESULTS
==================================================
Total Loss: 3.653041
  - ZINB NLL: 1.692225
Total number of masked values: 8894400
  - Number of values with pi >= 0.5: 364120
  - Number of values with pi < 0.5: 8530280
  - Masked Reconstruction: 1.960816
Mean Absolute Error (MAE): 2.136503
R² Score: 0.381241
==================================================

Using raw counts for evaluation metrics.
29648000 13164927 16483073
Zero imputation: 13,164,927 actual zeros, 12,105,971 imputed (92.0%), 160,250 false structural zeros
Number of masked values: 8894400
[0.6  0.   2.6  ... 5.4  0.55 0.5 ]
[1.036306  1.0825752 1.0025957 ... 1.3558905 1.1893667 1.011919 ]
[6.016825e-05 1.000000e-05 1.000000e-05 ... 1.000000e-05 1.000000e-05
 1.000000e-05]
Masked values analysis: 8894400 masked positions, MAE=1.9991, R²=0.5175
ZINB metrics saved to /workspace/AE/results/training/this_is_exp+1/this_is_exp+1_ZINBAE_20260119_140641_conv_test_metrics.json
ZINB-specific plots saved to /workspace/AE/results/training/this_is_exp+1/

==================================================
TEST RESULTS
==================================================
Total Loss: 4.582814
  - ZINB NLL: 1.904582
Total number of masked values: 1176000
  - Number of values with pi >= 0.5: 30635
  - Number of values with pi < 0.5: 1145365
  - Masked Reconstruction: 2.678232
Mean Absolute Error (MAE): 2.677202
R² Score: 0.079075
==================================================

Using raw counts for evaluation metrics.
3920000 1662825 2257175
Zero imputation: 1,662,825 actual zeros, 1,581,104 imputed (95.1%), 20,599 false structural zeros
Number of masked values: 1176000
[22.65      4.2875   23.900555 ...  0.        0.        0.      ]
[1.0170223 1.0317758 1.0025468 ... 1.2509161 1.2073282 1.1381273]
[1.4764108e-05 9.9999997e-06 9.9999997e-06 ... 1.8270211e-03 1.0368500e-04
 9.9999997e-06]
Masked values analysis: 1176000 masked positions, MAE=2.7005, R²=0.0764
ZINB metrics saved to /workspace/AE/results/testing/this_is_exp+1/this_is_exp+1_ZINBAE_20260119_141433_conv_test_metrics.json
ZINB-specific plots saved to /workspace/AE/results/testing/this_is_exp+1/
