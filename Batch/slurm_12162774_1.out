Task 1: Training ZINBVAE
Device: cuda
Loading training data from: Data/processed_data/BinSize10_MovingAvgFalse_ZINB_train_data.npy
Applied denoising with 30.0% masking
Loading test data from: Data/processed_data/BinSize10_MovingAvgFalse_ZINB_test_data.npy
Applied denoising with 30.0% masking
Number of training samples: 14824
============================================================
TRAINING ZINB VARIATIONAL AUTOENCODER (ZINBVAE)
============================================================
Epoch [1/100], Total Loss: 4.4215, Recon: 1.8064, KL: 0.1528, Masked: 2.4622
Epoch [2/100], Total Loss: 3.9512, Recon: 1.7228, KL: 0.0134, Masked: 2.2151
Epoch [3/100], Total Loss: 3.9261, Recon: 1.7105, KL: 0.0113, Masked: 2.2042
Epoch [4/100], Total Loss: 3.9331, Recon: 1.7162, KL: 0.0174, Masked: 2.1995
Epoch [5/100], Total Loss: 3.9083, Recon: 1.6955, KL: 0.0098, Masked: 2.2029
Epoch [6/100], Total Loss: 3.8849, Recon: 1.6834, KL: 0.0098, Masked: 2.1917
Epoch [7/100], Total Loss: 3.8721, Recon: 1.6815, KL: 0.0096, Masked: 2.1810
Epoch [8/100], Total Loss: 3.8463, Recon: 1.6700, KL: 0.0123, Masked: 2.1640
Epoch [9/100], Total Loss: 3.8350, Recon: 1.6633, KL: 0.0146, Masked: 2.1571
Epoch [10/100], Total Loss: 3.8197, Recon: 1.6581, KL: 0.0151, Masked: 2.1465
Epoch [11/100], Total Loss: 3.8143, Recon: 1.6553, KL: 0.0149, Masked: 2.1442
Epoch [12/100], Total Loss: 3.8053, Recon: 1.6544, KL: 0.0151, Masked: 2.1358
Epoch [13/100], Total Loss: 3.7921, Recon: 1.6480, KL: 0.0155, Masked: 2.1286
Epoch [14/100], Total Loss: 3.7871, Recon: 1.6451, KL: 0.0163, Masked: 2.1257
Epoch [15/100], Total Loss: 3.7755, Recon: 1.6424, KL: 0.0160, Masked: 2.1170
Epoch [16/100], Total Loss: 3.7578, Recon: 1.6370, KL: 0.0167, Masked: 2.1041
Epoch [17/100], Total Loss: 3.7561, Recon: 1.6334, KL: 0.0173, Masked: 2.1053
Epoch [18/100], Total Loss: 3.7520, Recon: 1.6326, KL: 0.0175, Masked: 2.1019
Epoch [19/100], Total Loss: 3.7442, Recon: 1.6268, KL: 0.0177, Masked: 2.0996
Epoch [20/100], Total Loss: 3.7308, Recon: 1.6237, KL: 0.0176, Masked: 2.0896
Epoch [21/100], Total Loss: 3.7251, Recon: 1.6222, KL: 0.0175, Masked: 2.0855
Epoch [22/100], Total Loss: 3.7190, Recon: 1.6203, KL: 0.0176, Masked: 2.0811
Epoch [23/100], Total Loss: 3.7087, Recon: 1.6171, KL: 0.0172, Masked: 2.0745
Epoch [24/100], Total Loss: 3.7028, Recon: 1.6150, KL: 0.0173, Masked: 2.0705
Epoch [25/100], Total Loss: 3.7033, Recon: 1.6144, KL: 0.0172, Masked: 2.0717
Epoch [26/100], Total Loss: 3.6918, Recon: 1.6116, KL: 0.0175, Masked: 2.0627
Epoch [27/100], Total Loss: 3.6832, Recon: 1.6091, KL: 0.0173, Masked: 2.0568
Epoch [28/100], Total Loss: 3.7043, Recon: 1.6084, KL: 0.0181, Masked: 2.0778
Epoch [29/100], Total Loss: 3.6678, Recon: 1.6043, KL: 0.0181, Masked: 2.0454
Epoch [30/100], Total Loss: 3.6570, Recon: 1.6009, KL: 0.0181, Masked: 2.0380
Epoch [31/100], Total Loss: 3.6579, Recon: 1.6006, KL: 0.0184, Masked: 2.0389
Epoch [32/100], Total Loss: 3.6428, Recon: 1.5974, KL: 0.0182, Masked: 2.0272
Epoch [33/100], Total Loss: 3.8139, Recon: 1.6079, KL: 0.0184, Masked: 2.1875
Epoch [34/100], Total Loss: 3.6359, Recon: 1.5944, KL: 0.0190, Masked: 2.0225
Epoch [35/100], Total Loss: 3.6288, Recon: 1.5925, KL: 0.0195, Masked: 2.0168
Epoch [36/100], Total Loss: 3.6166, Recon: 1.5891, KL: 0.0196, Masked: 2.0079
Epoch [37/100], Total Loss: 3.6094, Recon: 1.5875, KL: 0.0195, Masked: 2.0024
Epoch [38/100], Total Loss: 3.5991, Recon: 1.5845, KL: 0.0199, Masked: 1.9947
Epoch [39/100], Total Loss: 3.5880, Recon: 1.5821, KL: 0.0196, Masked: 1.9862
Epoch [40/100], Total Loss: 3.5832, Recon: 1.5800, KL: 0.0194, Masked: 1.9838
Epoch [41/100], Total Loss: 3.5835, Recon: 1.5808, KL: 0.0201, Masked: 1.9826
Epoch [42/100], Total Loss: 3.5707, Recon: 1.5781, KL: 0.0196, Masked: 1.9730
Epoch [43/100], Total Loss: 3.5685, Recon: 1.5776, KL: 0.0200, Masked: 1.9709
Epoch [44/100], Total Loss: 3.5622, Recon: 1.5760, KL: 0.0196, Masked: 1.9666
Epoch [45/100], Total Loss: 3.5651, Recon: 1.5741, KL: 0.0195, Masked: 1.9715
Epoch [46/100], Total Loss: 3.5444, Recon: 1.5728, KL: 0.0195, Masked: 1.9520
Epoch [47/100], Total Loss: 3.5393, Recon: 1.5711, KL: 0.0196, Masked: 1.9487
Epoch [48/100], Total Loss: 3.5340, Recon: 1.5706, KL: 0.0195, Masked: 1.9439
Epoch [49/100], Total Loss: 3.5225, Recon: 1.5681, KL: 0.0194, Masked: 1.9350
Epoch [50/100], Total Loss: 3.5266, Recon: 1.5679, KL: 0.0196, Masked: 1.9391
Epoch [51/100], Total Loss: 3.5158, Recon: 1.5667, KL: 0.0197, Masked: 1.9294
Epoch [52/100], Total Loss: 3.5109, Recon: 1.5657, KL: 0.0194, Masked: 1.9258
Epoch [53/100], Total Loss: 3.5040, Recon: 1.5639, KL: 0.0197, Masked: 1.9204
Epoch [54/100], Total Loss: 3.4921, Recon: 1.5620, KL: 0.0195, Masked: 1.9107
Epoch [55/100], Total Loss: 3.4935, Recon: 1.5624, KL: 0.0193, Masked: 1.9118
Epoch [56/100], Total Loss: 3.4894, Recon: 1.5608, KL: 0.0195, Masked: 1.9091
Epoch [57/100], Total Loss: 3.4817, Recon: 1.5599, KL: 0.0195, Masked: 1.9023
Epoch [58/100], Total Loss: 3.4709, Recon: 1.5577, KL: 0.0194, Masked: 1.8938
Epoch [59/100], Total Loss: 3.4807, Recon: 1.5587, KL: 0.0197, Masked: 1.9023
Epoch [60/100], Total Loss: 3.4696, Recon: 1.5572, KL: 0.0201, Masked: 1.8923
Epoch [61/100], Total Loss: 3.4633, Recon: 1.5558, KL: 0.0197, Masked: 1.8878
Epoch [62/100], Total Loss: 3.4555, Recon: 1.5546, KL: 0.0197, Masked: 1.8812
Epoch [63/100], Total Loss: 3.4566, Recon: 1.5541, KL: 0.0197, Masked: 1.8828
Epoch [64/100], Total Loss: 3.4474, Recon: 1.5535, KL: 0.0197, Masked: 1.8741
Epoch [65/100], Total Loss: 3.4437, Recon: 1.5519, KL: 0.0198, Masked: 1.8720
Epoch [66/100], Total Loss: 3.4406, Recon: 1.5520, KL: 0.0196, Masked: 1.8690
Epoch [67/100], Total Loss: 3.4363, Recon: 1.5507, KL: 0.0199, Masked: 1.8658
Epoch [68/100], Total Loss: 3.4287, Recon: 1.5494, KL: 0.0198, Masked: 1.8596
Epoch [69/100], Total Loss: 3.4252, Recon: 1.5492, KL: 0.0198, Masked: 1.8562
Epoch [70/100], Total Loss: 3.4216, Recon: 1.5484, KL: 0.0198, Masked: 1.8535
Epoch [71/100], Total Loss: 3.4237, Recon: 1.5477, KL: 0.0199, Masked: 1.8561
Epoch [72/100], Total Loss: 3.4100, Recon: 1.5459, KL: 0.0198, Masked: 1.8443
Epoch [73/100], Total Loss: 3.4140, Recon: 1.5460, KL: 0.0200, Masked: 1.8480
Epoch [74/100], Total Loss: 3.4069, Recon: 1.5453, KL: 0.0200, Masked: 1.8416
Epoch [75/100], Total Loss: 3.4030, Recon: 1.5446, KL: 0.0199, Masked: 1.8384
Epoch [76/100], Total Loss: 3.3975, Recon: 1.5432, KL: 0.0199, Masked: 1.8343
Epoch [77/100], Total Loss: 3.3919, Recon: 1.5422, KL: 0.0202, Masked: 1.8296
Epoch [78/100], Total Loss: 3.3898, Recon: 1.5414, KL: 0.0200, Masked: 1.8283
Epoch [79/100], Total Loss: 3.3888, Recon: 1.5419, KL: 0.0203, Masked: 1.8266
Epoch [80/100], Total Loss: 3.3850, Recon: 1.5410, KL: 0.0203, Masked: 1.8237
Epoch [81/100], Total Loss: 3.3845, Recon: 1.5411, KL: 0.0204, Masked: 1.8230
Epoch [82/100], Total Loss: 3.3754, Recon: 1.5391, KL: 0.0203, Masked: 1.8161
Epoch [83/100], Total Loss: 3.3700, Recon: 1.5384, KL: 0.0203, Masked: 1.8113
Epoch [84/100], Total Loss: 3.3710, Recon: 1.5378, KL: 0.0206, Masked: 1.8126
Epoch [85/100], Total Loss: 3.3630, Recon: 1.5370, KL: 0.0204, Masked: 1.8056
Epoch [86/100], Total Loss: 3.3607, Recon: 1.5361, KL: 0.0204, Masked: 1.8041
Epoch [87/100], Total Loss: 3.3571, Recon: 1.5362, KL: 0.0204, Masked: 1.8005
Epoch [88/100], Total Loss: 3.3522, Recon: 1.5349, KL: 0.0203, Masked: 1.7971
Epoch [89/100], Total Loss: 3.3500, Recon: 1.5347, KL: 0.0205, Masked: 1.7948
Epoch [90/100], Total Loss: 3.3538, Recon: 1.5348, KL: 0.0205, Masked: 1.7985
Epoch [91/100], Total Loss: 3.3453, Recon: 1.5334, KL: 0.0206, Masked: 1.7913
Epoch [92/100], Total Loss: 3.3413, Recon: 1.5329, KL: 0.0205, Masked: 1.7879
Epoch [93/100], Total Loss: 3.3403, Recon: 1.5328, KL: 0.0206, Masked: 1.7870
Epoch [94/100], Total Loss: 3.3383, Recon: 1.5320, KL: 0.0205, Masked: 1.7858
Epoch [95/100], Total Loss: 3.3294, Recon: 1.5304, KL: 0.0206, Masked: 1.7784
Epoch [96/100], Total Loss: 3.3286, Recon: 1.5301, KL: 0.0208, Masked: 1.7778
Epoch [97/100], Total Loss: 3.3240, Recon: 1.5292, KL: 0.0206, Masked: 1.7741
Epoch [98/100], Total Loss: 3.3218, Recon: 1.5288, KL: 0.0206, Masked: 1.7723
Epoch [99/100], Total Loss: 3.3240, Recon: 1.5290, KL: 0.0207, Masked: 1.7743
Epoch [100/100], Total Loss: 3.3189, Recon: 1.5283, KL: 0.0207, Masked: 1.7700
Training loss plots saved to /workspace/AE/results/training/this_is_exp+1/
Training losses saved to /workspace/AE/results/training/this_is_exp+1/this_is_exp+1_ZINBVAE_20260119_141338_conv_training_losses.json

==================================================
EVALUATING ON TRAINING DATA
==================================================

==================================================
TEST RESULTS
==================================================
Total Loss: 3.310479
  - ZINB NLL: 1.514646
Total number of masked values: 8894400
  - Number of values with pi >= 0.5: 555
  - Number of values with pi < 0.5: 8893845
  - KL Divergence: 0.020462
  - Masked Reconstruction: 1.775371
Mean Absolute Error (MAE): 1.924398
R² Score: 0.318246
==================================================

Using raw counts for evaluation metrics.
29648000 13164927 16483073
Zero imputation: 13,164,927 actual zeros, 13,162,929 imputed (100.0%), 83 false structural zeros
Number of masked values: 8894400
[0.        0.        0.        ... 1.5722222 1.6333333 0.83     ]
[0.03628044 0.04481553 0.04189576 ... 1.5977426  1.1089772  0.48827595]
[1.e-05 1.e-05 1.e-05 ... 1.e-05 1.e-05 1.e-05]
Masked values analysis: 8894400 masked positions, MAE=1.7754, R²=0.4134
ZINB metrics saved to /workspace/AE/results/training/this_is_exp+1/this_is_exp+1_ZINBVAE_20260119_141341_conv_test_metrics.json
ZINB-specific plots saved to /workspace/AE/results/training/this_is_exp+1/

==================================================
TEST RESULTS
==================================================
Total Loss: 4.301232
  - ZINB NLL: 1.794661
Total number of masked values: 1176000
  - Number of values with pi >= 0.5: 14
  - Number of values with pi < 0.5: 1175986
  - KL Divergence: 0.019747
  - Masked Reconstruction: 2.486824
Mean Absolute Error (MAE): 2.486209
R² Score: 0.049592
==================================================

Using raw counts for evaluation metrics.
3920000 1662825 2257175
Zero imputation: 1,662,825 actual zeros, 1,662,810 imputed (100.0%), 38 false structural zeros
Number of masked values: 1176000
[0.1 0.1 0.  ... 2.8 0.2 1.3]
[0.23329884 0.3682856  0.19574012 ... 0.46885377 0.88407737 0.34124348]
[1.e-05 1.e-05 1.e-05 ... 1.e-05 1.e-05 1.e-05]
Masked values analysis: 1176000 masked positions, MAE=2.4868, R²=0.0474
ZINB metrics saved to /workspace/AE/results/testing/this_is_exp+1/this_is_exp+1_ZINBVAE_20260119_142311_conv_test_metrics.json
ZINB-specific plots saved to /workspace/AE/results/testing/this_is_exp+1/
